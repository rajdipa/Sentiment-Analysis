{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import cleaning\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer as rt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,csc_matrix,hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fName, colName):\n",
    "    data = pd.read_csv(fName)\n",
    "    return data[colName]\n",
    "\n",
    "# Output result of cleaning into a txt file, by the name of fname\n",
    "# resultlist is a list of lists, the items in the inner list are strings\n",
    "def print_result(fName, resultlist):\n",
    "    with open(fName, 'w') as f:\n",
    "        for l in result:\n",
    "            f.write(\"%s\\n\" % l)\n",
    "    f.close()\n",
    "\n",
    "# Dump the preprocessed data into a pickle file\n",
    "def generate_cleaned_file(instances, labels):\n",
    "    col = [\"Phrases\", \"Sentiment\"]\n",
    "    i = range(len((instances)))\n",
    "    dataframe = pd.DataFrame(columns = col, index = i)\n",
    "    dataframe[\"Phrases\"] = instances\n",
    "    dataframe[\"Sentiment\"] = labels\n",
    "    pkfile = open('preprocessed_train_data.pkl', 'wb')\n",
    "    rick.dump(dataframe, pkfile)\n",
    "    pkfile.close()\n",
    "\n",
    "# Open the pickle file\n",
    "def open_cleaned_file(fileName):\n",
    "    pkl = open(fileName, 'rb')\n",
    "    result = rick.load(pkl)\n",
    "    pkl.close()\n",
    "    return result\n",
    "\n",
    "# Tokenize and filter out non-alphanumeric characters.\n",
    "def tokenize_data(d):\n",
    "    # Initialize RegExp tokenizer.\n",
    "    tokenizer = rt(r'\\w+')\n",
    "    # Make all words lowercase.\n",
    "    d = (phrase.lower() for phrase in d)\n",
    "    d = (tokenizer.tokenize(phrase) for phrase in d)\n",
    "    tokenized = []\n",
    "    for phrase in d:\n",
    "        tokenized.append(phrase)\n",
    "    # print_result('cleaned.txt', result)\n",
    "    return tokenized\n",
    "\n",
    "def untokenize(texts):\n",
    "    docs = []\n",
    "    for doc in texts:\n",
    "        temp = \"\"\n",
    "        for word in doc:\n",
    "            temp += word + \" \"\n",
    "        docs.append(temp)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(d):\n",
    "    # Initialize stopwords list: remove articles, conjunctions, prepositions, pronouns.\n",
    "    # stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    # Additional non-sentimental words to filter out\n",
    "    custom_words = ['genre', 'film', 'movie']\n",
    "    stop_words = set(stop_words).union(set(custom_words))\n",
    "    # Second cleaning\n",
    "    # Initialize dictionary of useful words from nltk corpus\n",
    "    good_words = set(list(words.words()))\n",
    "    result = []\n",
    "    for phrase in d:\n",
    "        temp = []\n",
    "        for word in phrase:\n",
    "            if word in good_words and not word in stop_words:\n",
    "                temp.append(word)\n",
    "        result.append(temp)\n",
    "    # print_result('secondclean.txt', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = read_data(\"train.csv\", \"Phrase\")\n",
    "labels = read_data(\"train.csv\", \"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = tokenize_data(phrases)\n",
    "result = filter_data(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2, ngram_range=(0, 30))\n",
    "X_train = vect.fit(untokenize(result)).transform(untokenize(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RAJ/anaconda/envs/py7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/RAJ/anaconda/envs/py7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/RAJ/anaconda/envs/py7/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = read_data(\"testset_1.csv\", \"Phrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phraseid = read_data(\"testset_1.csv\", \"PhraseId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print (type(test_phraseid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = tokenize_data(x_test)\n",
    "result_test = filter_data(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform((untokenize(result_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_df(test_phraseid,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'PhraseId':np.array(test_phraseid),'Sentiment':np.array(y)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(outfile,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
